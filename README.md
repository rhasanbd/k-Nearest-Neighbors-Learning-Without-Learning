# k-Nearest Neighbors - Learning Without Learning

In this notebook series on the k-Nearest Neighbors (k-NN), we introduce the k-NN model and investigate various aspects of performing machine learning (ML) using the k-NN model.

- k-NN 0: How to use Scikit-Learn to create a k-NN **classifier** model
- k-NN 1: Visualize k-NN decision boundary
- k-NN 2: Introduce various steps of Machine Learning for performing binary classification using the k-NN model. The steps include:

  -- Creating a test dataset
  
  -- Creating the k-NN classifier model

  -- Model selection via hyperparameter tuning

  -- Understanding the probabilistic prediction of the k-NN model

  -- Evaluate a k-NN model using accuracy, precision, recall, F1 score, and confusion matrix
  
  -- Creating the Receiver Operating Characteristic (ROC) curve
 
  -- Creating the Precision-Recall curve & computing area under the curve (AUC) score

  -- Determining the optimal threshold

  -- Improve a model's performance by using its optimal threshold
  
- k-NN 3: Introduce various steps of Machine Learning for performing multi-class classification using the k-NN model 

- k-NN 4: Effect of data scaling on k-NN classification

- k-NN 5: Investigate the non-linear decision boundary of the k-NN classifier

- k-NN 6: Investigate the curse of dimensionality (multi-class classification of the MNIST handwritten digits dataset)

- k-NN 7: How to use Scikit-Learn to create a k-NN **regressor** model

- k-NN 8: Perform regression on a housing dataset using Scikit-Learn's k-NN regressor



Note: If GitHub is unable to render a Jupyter Notebook, copy the link of the notebook and enter it into the nbviewer:
https://nbviewer.jupyter.org/
